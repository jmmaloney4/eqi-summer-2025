\documentclass[11pt,reqno]{amsart}

% -----------------------------------------------------------
% PACKAGES
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{tikz-cd} % CTAN: pgf
\usepackage{geometry}
\usepackage{braket}
\usepackage{xcolor}
\usepackage{mdframed} % CTAN: zref, needspace
\geometry{margin=1in}

% -----------------------------------------------------------
% THEOREM STYLES
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem*{note}{Note}

% TODO environment for callouts with mdframed
\definecolor{todobackground}{rgb}{1.0,0.95,0.8}
\definecolor{todoframe}{rgb}{0.8,0.6,0.2}

\newmdenv[
  backgroundcolor=todobackground,
  linecolor=todoframe,
  linewidth=2pt,
  topline=true,
  bottomline=true,
  leftline=true,
  rightline=true,
  innertopmargin=8pt,
  innerbottommargin=8pt,
  innerleftmargin=10pt,
  innerrightmargin=10pt,
  skipabove=12pt,
  skipbelow=12pt,
  frametitle={\textbf{TODO}},
  frametitlefont=\bfseries\color{todoframe}
]{todo}

% -----------------------------------------------------------
% CUSTOM COMMANDS
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}

% -----------------------------------------------------------
% DOCUMENT START
\begin{document}

\title{Notes on \textit{Elements of Quantitative Investing} by Paleologo}
\author{Jack Maloney}
\email{jmmaloney4@gmail.com}
\date{\today}

\maketitle

\tableofcontents

\section{Prerequisites and Return Space}

\subsection{Probability Spaces and Measurable Functions}

\begin{definition}
	A \emph{probability space} is a triple $(\Omega, \mathcal{F}, P)$ where $\Omega$ is a set, $\mathcal{F}$ is a $\sigma$-algebra of subsets of $\Omega$, and $P: \mathcal{F} \to [0,1]$ is a probability measure with $P(\Omega) = 1$.
\end{definition}

The probability space is the fundamental object of probability theory. Many sets can be probability spaces, but one in particular is the \emph{physical measure} space, usually written as $(\Omega, \mathcal{F}, P)$.
This is the space of all possible states of the world, with a probability distribution that reflects our current knowledge \cite{clayton2023bernoulli}.
As you can imagine, there is not usually a great deal we can say about the physical measure space with any precision.
We can, however, make statements about certain random variables--functions from the physical measure space to the real numbers.
Random variables often model some obsevable quantity, such as the price of a stock, the weather, or the outcome of a coin flip.

\begin{definition}
	A \emph{random variable} is a measurable function $X: (\Omega, \mathcal{F}) \to (\mathbb{R}, \mathcal{B}(\mathbb{R}))$, where $\mathcal{B}(\mathbb{R})$ denotes the Borel $\sigma$-algebra on $\mathbb{R}$.

	Recall that a function $f: (\Omega, \mathcal{F}) \to (\mathbb{R}, \mathcal{B}(\mathbb{R}))$ is measurable if $f^{-1}(B) \in \mathcal{F}$ for all $B \in \mathcal{B}(\mathbb{R})$.
\end{definition}

\begin{example}
	Let \( X : \Omega \rightarrow \Set{1,0} \) be the random variable representing a particular coin toss. This means \(X\)  a map that assigns to each state of the universe \( \omega \in \Omega \) the value \(1\) if the coin lands heads and \(0\) if the coin lands tails.
	\begin{align*}
		\omega \mapsto \begin{cases}
			               1 & \text{if coin lands heads in state } \omega \\
			               0 & \text{if coin lands tails in state } \omega
		               \end{cases}
	\end{align*}
	For simplicity of modeling, we assume all states of the universe have a coin toss, that ends in either heads or tails, and so this function \(X\) is well-defined.
	Otherwise, we would have to extend the range to handle any exceptional cases.

	To assert that \(X\) is a random variable, we must claim that the set of heads \(X^{-1}(\Set{1})\) and the set of tails \(X^{-1}(\Set{0})\) are both measurable sets in the \(\sigma\)-algebra of the physical measure space, \(\mathcal{F}\).
\end{example}

What makes random variables useful is that they allow us to move from studying the physical measure space, to studying the real numbers under the distribution of the random variable.

\begin{definition}
	The \emph{distribution} of a random variable \(X\) is a measure defined on the real numbers, the pushforward of the physical measure \(P\) by \(X\),
	\begin{align*}
		X_{*}P (E) := P \left( X^{-1}(E) \right) \quad \text{for } E \in \mathcal{B}(\mathbb{R}).
	\end{align*}
\end{definition}

\begin{example}
	Define a random variable \(Y: \Omega \rightarrow \mathbb{R}\) representing the price of a particular security at market close on a given day.
	Then the distribution of \(Y\), \(Y_{*}P (E) := P \left( Y^{-1}(E) \right)\), is a probability measure on \(\mathbb{R}\) that describes the probability that the security closes at a particular price.
\end{example}

Most random variables encountered in practice have a distribution with the additional nice property of being represented by a density function.

\begin{definition}
	If a random variable is absolutely continuous with respect to Lebesgue measure, i.e. \(X_{*}P \ll \lambda\), then we say the random variable has a \emph{probability density function} (unique up to Lebesgue-null sets)
	\begin{align*}
		f_X = \frac{d X_{*}P}{dx},
	\end{align*}
	the Radon-Nikodym derivative of the distribution with respect to Lebesgue measure.
\end{definition}

It's often useful to go the other way, and define a random variable by it's density function. Given \(f_X: \mathbb{R} \rightarrow \mathbb{R}\), we can define probability measure \(X_{*}P\) by

\begin{align*}
	X_{*}P (E) := \int_E f_X(x) \, dx \quad \text{for } E \in \mathcal{B}(\mathbb{R}).
\end{align*}

\subsection{Integration and Expectation}

\begin{definition}
	The \emph{expectation} (or \emph{mean}) of a random variable $X$ is the Lebesgue integral
	\begin{align} \label{def:expectation}
		\mathbb{E}[X]
		 & := \int_\Omega X(\omega) \, dP(\omega)
		= \int_{\mathbb{R}} x \, dX_*P(x)
		= \int_{\mathbb{R}} x \, f_X(x) dx.
	\end{align}
	provided the integral exists in the extended real line $\overline{\mathbb{R}} := \mathbb{R} \cup \Set{\pm \infty}$. (The last equality is of course only valid in the case that \(f_X\) exists.)
\end{definition}

\begin{remark}[Change of Variables Formula]
	The equality in \eqref{def:expectation} follows from the general change of variables formula. For measurable spaces $(X, \mathcal{A})$ and $(Y, \mathcal{B})$, a measure $\mu$ on $(X, \mathcal{A})$, a measurable function $f: X \to Y$, and a measurable function $g: Y \to \mathbb{R}$, we have
	\begin{align*}
		\int_X (g \circ f) \, d\mu = \int_Y g \, d(f_*\mu).
	\end{align*}
	In our case, taking $f = X: \Omega \to \mathbb{R}$ and $g = \text{id}: \mathbb{R} \to \mathbb{R}$ where $\text{id}(x) = x$, we get
	\begin{align*}
		X(\omega) = (\text{id} \circ X)(\omega) = \text{id}(X(\omega)),
	\end{align*}
	and therefore
	\begin{align} \label{eq:change-of-variables}
		\int_\Omega X(\omega) \, dP(\omega)
		= \int_\Omega (\text{id} \circ X)(\omega) \, dP(\omega)
		= \int_{\mathbb{R}} \text{id}(x) \, dX_*P(x)
		= \int_{\mathbb{R}} x \, dX_*P(x).
	\end{align}
\end{remark}

Another way of looking at the expectation is through the tail-sum or "layer cake" formula. First, we need to define the CDF and the tail (function) of a random variable.

\begin{definition}
	The \emph{cumulative distribution function} (CDF) of a random variable $X$ is the function $F_X: \mathbb{R} \to [0,1]$ defined by
	\begin{align*}
		F_X(x)
		 & = P(X \leq x)                                                                                                                                           \\
		 & = \int \mathbf{1}_{\Set{\omega \in \Omega | X(\omega) \leq x}}(\omega) \, dP(\omega)                                                                    \\
		 & = \int \mathbf{1}_{(-\infty, x]} \circ X (\omega) \, dP(\omega)                                                                                         \\
		 & = \int_{-\infty}^x dX_*P(x).                                                         &  & \text{by change of variables, \eqref{eq:change-of-variables}}
	\end{align*}
\end{definition}


\begin{definition}
	The \emph{tail} or \emph{survival function} of a random variable $X$ is the function $\bar{F}_X: \mathbb{R} \to [0,1]$ defined by
	\begin{align*}
		\bar{F}_X(x) = P(X > x) = \int_x^\infty dX_*P(x) = 1 - F_X(x).
	\end{align*}
\end{definition}

\begin{proposition}[Tail-Sum Formula]
	For a random variable $X$, the expectation can be expressed as
	\begin{align*}
		\mathbb{E}[X] = \int_0^\infty P(X > t) \, dt - \int_0^\infty P(X \leq -t) \, dt.
	\end{align*}
	In particular, if $X \geq 0$ almost surely, then
	\begin{align*}
		\mathbb{E}[X] = \int_0^\infty P(X > t) \, dt.
	\end{align*}
\end{proposition}

\begin{proof}
	We prove the non-negative case first. For $X \geq 0$, we use Fubini's theorem:
	\begin{align*}
		\mathbb{E}[X] & = \int_\Omega X(\omega) \, dP(\omega)                                                                             \\
		              & = \int_\Omega \left( \int_0^{X(\omega)} dt \right) dP(\omega)                                                     \\
		              & = \int_\Omega \int_0^\infty \mathbf{1}_{[0, X(\omega)]}(t) \, dt \, dP(\omega)                                    \\
		              & = \int_0^\infty \int_\Omega \mathbf{1}_{[0, X(\omega)]}(t) \, dP(\omega) \, dt            &  & \text{(by Fubini)} \\
		              & = \int_0^\infty \int_\Omega \mathbf{1}_{\{X(\omega) \geq t\}}(\omega) \, dP(\omega) \, dt                         \\
		              & = \int_0^\infty P(X \geq t) \, dt.
	\end{align*}
	Note that $P(X \geq t) = P(X > t)$ when $X$ has a continuous distribution, so we often write the final result as $\int_0^\infty P(X > t) \, dt$.

	For the general case, write $X = X^+ - X^-$ where $X^+ = \max(X, 0)$ and $X^- = \max(-X, 0)$. Then:
	\begin{align*}
		\mathbb{E}[X] & = \mathbb{E}[X^+] - \mathbb{E}[X^-]                                \\
		              & = \int_0^\infty P(X^+ > t) \, dt - \int_0^\infty P(X^- > t) \, dt  \\
		              & = \int_0^\infty P(X > t) \, dt - \int_0^\infty P(-X > t) \, dt     \\
		              & = \int_0^\infty P(X > t) \, dt - \int_0^\infty P(X < -t) \, dt     \\
		              & = \int_0^\infty P(X > t) \, dt - \int_0^\infty P(X \leq -t) \, dt.
	\end{align*}
\end{proof}

\subsection{\(L^p\) Spaces and Moments}

For many purposes, it's useful random variables via their relationship to other random variables within some structure. The most common of these structures is the \(L^p\) space.

\begin{definition}
	For $1 \leq p < \infty$, the space $L^p(\Omega, \mathcal{F}, P)$ consists of equivalence classes of measurable functions $X$ with finite $p$-th moment (\(L^p\)-norm):
	\begin{align*}
		\|X\|_p := \left( \int_\Omega |X|^p \, dP \right)^{1/p} < \infty.
	\end{align*}
\end{definition}

As we alluded above, the \(L^p\) spaces are a family of spaces of random variables that are equipped with a norm.

\begin{proposition}[Minkowski's Inequality]
	Let $1 \leq p < \infty$. Then for any $X, Y \in L^p$,
	\begin{align*}
		\|X + Y\|_p \leq \|X\|_p + \|Y\|_p.
	\end{align*}
\end{proposition}

\subsection{Hölder's Inequality (Interlude)}

We now state and prove several central results in analysis, culminating in Hölder's inequality, a generalization of the triangle inequality for \(L^p\) spaces, which appears repeatedly in the theory.

\begin{lemma}[Jensen's Inequality]
	Let $f: (a,b) \to \mathbb{R}$ be a concave function on an interval $(a,b)$. For any $x_1, x_2 \in (a,b)$ and $\lambda \in [0,1]$,
	\begin{align*}
		f(\lambda x_1 + (1-\lambda) x_2) \geq \lambda f(x_1) + (1-\lambda) f(x_2).
	\end{align*}
	If $f$ is strictly concave, equality holds if and only if $x_1 = x_2$ or $\lambda \in \{0,1\}$.
\end{lemma}

\begin{proof}
	By definition, $f$ is concave if its second derivative $f''(x) \leq 0$ on $(a,b)$. Let $x = \lambda x_1 + (1-\lambda) x_2$ and consider the function
	\begin{align*}
		g(t) = f(x + t) - f(x) - tf'(x)
	\end{align*}
	for $t$ such that $x + t \in (a,b)$. Then $g'(t) = f'(x + t) - f'(x)$ and $g''(t) = f''(x + t) \leq 0$, so $g'$ is decreasing. Since $g'(0) = 0$, we have $g'(t) \leq 0$ for $t \geq 0$ and $g'(t) \geq 0$ for $t \leq 0$. This means $g$ has a global maximum at $t = 0$, so $g(t) \leq g(0) = 0$ for all valid $t$.

	Now, taking $t_1 = x_1 - x = (1-\lambda)(x_1 - x_2)$ and $t_2 = x_2 - x = -\lambda(x_1 - x_2)$:
	\begin{align*}
		f(x_1) - f(x) - (x_1 - x)f'(x) & \leq 0 \\
		f(x_2) - f(x) - (x_2 - x)f'(x) & \leq 0
	\end{align*}
	Multiplying the first inequality by $\lambda$ and the second by $(1-\lambda)$, then adding:
	\begin{align*}
		\lambda f(x_1) + (1-\lambda) f(x_2) - f(x) - [\lambda(x_1 - x) + (1-\lambda)(x_2 - x)]f'(x) \leq 0
	\end{align*}
	Since $\lambda(x_1 - x) + (1-\lambda)(x_2 - x) = 0$, we get the desired inequality.
\end{proof}

\begin{lemma}[Young's Inequality]
	Let $1 < p < \infty$ and $\frac{1}{p} + \frac{1}{q} = 1$. Then for any $a, b \geq 0$,
	\begin{align*}
		ab \leq \frac{a^p}{p} + \frac{b^q}{q}.
	\end{align*}
	Equality holds if and only if $a^p = b^q$.
\end{lemma}

\begin{proof}
	If $a = 0$ or $b = 0$, the inequality is trivial. Assume $a, b > 0$. Taking logarithms, we need to show:
	\begin{align*}
		\log(a) + \log(b) \leq \log\left(\frac{a^p}{p} + \frac{b^q}{q}\right).
	\end{align*}
	Since $\log$ is concave and $\frac{1}{p} + \frac{1}{q} = 1$, by Jensen's inequality:
	\begin{align*}
		\frac{1}{p}\log(a^p) + \frac{1}{q}\log(b^q) & \leq \log\left(\frac{1}{p} \cdot a^p + \frac{1}{q} \cdot b^q\right) \\
		                                            & = \log\left(\frac{a^p}{p} + \frac{b^q}{q}\right).
	\end{align*}
	But $\frac{1}{p}\log(a^p) + \frac{1}{q}\log(b^q) = \log(a) + \log(b)$, which gives the desired inequality.

	For the equality condition, Jensen's inequality is an equality if and only if $a^p = b^q$.
\end{proof}

\begin{theorem}[Hölder's Inequality]
	Let $1 \leq p, q \leq \infty$ with $\frac{1}{p} + \frac{1}{q} = 1$. Then for any $X \in L^p$ and $Y \in L^q$,
	\begin{align*}
		|\mathbb{E}[XY]| \leq \|X\|_p \|Y\|_q.
	\end{align*}
	Equivalently, $\|XY\|_1 \leq \|X\|_p \|Y\|_q$.
\end{theorem}

\begin{proof}
	We may assume $\|X\|_p > 0$ and $\|Y\|_q > 0$ (otherwise the inequality is trivial). For the case $1 < p < \infty$, define normalized variables:
	\begin{align*}
		\tilde{X} = \frac{|X|}{\|X\|_p}, \quad \tilde{Y} = \frac{|Y|}{\|Y\|_q}.
	\end{align*}
	Then $\|\tilde{X}\|_p = \|\tilde{Y}\|_q = 1$. By Young's inequality, for $a, b \geq 0$:
	\begin{align*}
		ab \leq \frac{a^p}{p} + \frac{b^q}{q}.
	\end{align*}
	Applying this with $a = \tilde{X}(\omega)$ and $b = \tilde{Y}(\omega)$:
	\begin{align*}
		\tilde{X}(\omega) \tilde{Y}(\omega) \leq \frac{[\tilde{X}(\omega)]^p}{p} + \frac{[\tilde{Y}(\omega)]^q}{q}.
	\end{align*}
	Taking expectations:
	\begin{align*}
		\mathbb{E}[\tilde{X} \tilde{Y}] \leq \frac{1}{p}\mathbb{E}[\tilde{X}^p] + \frac{1}{q}\mathbb{E}[\tilde{Y}^q] = \frac{1}{p} + \frac{1}{q} = 1.
	\end{align*}
	Substituting back:
	\begin{align*}
		\mathbb{E}\left[\frac{|X|}{\|X\|_p} \cdot \frac{|Y|}{\|Y\|_q}\right] \leq 1,
	\end{align*}
	which gives
	\begin{align*}
		\mathbb{E}[|XY|] \leq \|X\|_p \|Y\|_q.
	\end{align*}
	Since $|\mathbb{E}[XY]| \leq \mathbb{E}[|XY|]$, the result follows.
\end{proof}

\begin{definition}[Conjugate Exponents]
	Let $1 \leq p < \infty$. The \emph{conjugate exponent} of $p$ is the number $q$ such that $\frac{1}{p} + \frac{1}{q} = 1$, namely $q = \frac{p}{p-1}$.
\end{definition}

\begin{definition}
	The space \(L^\infty(\Omega, \mathcal{F}, P)\) consists of equivalence classes of \(P\)-almost everywhere equal measurable functions $X$ with finite essential (\(P\)-almost everywhere) supremum, otherwise known as the \(L^\infty\)-norm:
	\begin{align*}
		\|X\|_\infty := \inf \{C < \infty : |X| \leq C \text{ a.s.}\}.
	\end{align*}
\end{definition}

In fact, by extending the definition of conjugate exponents to let $\infty$ be the conjugate of \(1\), it is easy to see that Hölder's inequality holds for all \(p, q \in [1, \infty]\).

Finally, because we are in a probability space, there are some nice inequalities between the \(L^p\) norms. This works because there are essentially two ways a function can fail to be in \(L^p\):
\begin{enumerate}
	\item It has spikes that grow too quickly. The larger \(p\) grows, the more sensitive the \(L^p\) norm is to these spikes. We say functions in \(L^{q}\) spaces are \emph{more smooth} than functions in \(L^{p}\) when \(q > p\).
	\item It's tails are too large. The smaller \(p\) is, the more sensitive the \(L^p\) norm is to the tails of a function. We say functions in \(L^{q}\) \emph{show more decay} than functions in \(L^{p}\) when \(q < p\).
\end{enumerate}
However, because the measure space is finite, the second case cannot happen. The tails can never decay too slowly that \(\|X\|_p\) is infinite. It's easy to see this because the constant function \(\mathbf{1}\) is in \(L^p\) for any \(p\).

\begin{proposition}
	Let \(P\) be a finite measure, i.e. \(P(\Omega) < \infty\). Then for \(1 \leq p < q < \infty\) and any $X \in L^p$,
	\begin{align*}
		\|X\|_q \leq \|X\|_p \cdot P(\Omega)^{(q-p)/(pq)}.
	\end{align*}
\end{proposition}

\begin{proof}
	Write $|X|^p = |X|^p \cdot \mathbf{1}$ and apply Hölder's inequality with conjugate exponents $r = \frac{q}{p} > 1$ and $s = \frac{q}{q-p}$. Note that $\frac{1}{r} + \frac{1}{s} = \frac{p}{q} + \frac{q-p}{q} = 1$.

	Then:
	\begin{align*}
		\mathbb{E}[|X|^p] & = \mathbb{E}[|X|^p \cdot \mathbf{1}]                               \\
		                  & \leq (\mathbb{E}[|X|^{pr}])^{1/r} (\mathbb{E}[\mathbf{1}^s])^{1/s} \\
		                  & = (\mathbb{E}[|X|^q])^{p/q} \cdot P(\Omega)^{(q-p)/q}.
	\end{align*}

	Rearranging:
	\begin{align*}
		(\mathbb{E}[|X|^q])^{p/q} \geq \frac{\mathbb{E}[|X|^p]}{P(\Omega)^{(q-p)/q}}.
	\end{align*}

	Taking the $(q/p)$-th power:
	\begin{align*}
		\mathbb{E}[|X|^q] \leq \left(\mathbb{E}[|X|^p]\right)^{q/p} \cdot P(\Omega)^{(q-p)/p}.
	\end{align*}

	Taking the $q$-th root:
	\begin{align*}
		\|X\|_q = (\mathbb{E}[|X|^q])^{1/q} \leq \left(\mathbb{E}[|X|^p]\right)^{1/p} \cdot P(\Omega)^{(q-p)/(pq)} = \|X\|_p \cdot P(\Omega)^{(q-p)/(pq)}.
	\end{align*}
\end{proof}

For probability measures, \(P(\Omega) = 1\), so the inequality simplifies to \(\|X\|_q \leq \|X\|_p\) for \(1 \leq p \leq q \leq \infty\).

\begin{corollary}[Containment of \(L^p\) Spaces]
	Let \(P\) be a finite measure. Then for \(1 \leq p < q \leq \infty\),
	\begin{align*}
		L^p \subseteq L^q.
	\end{align*}
\end{corollary}

\subsection{Centered Random Variables, \(L^p_0\)}

Often, in probability theory, we are interested in centered moments. To study these formally, it we introduce the space of centered random variables, \(L^p_0\).

\begin{definition}
	The space \(L^p_0(\Omega, \mathcal{F}, P)\) consists of equivalence classes of measurable functions $X$ with finite $p$-th moment and zero expectation:
	\begin{align*}
		\|X\|_p := \left( \int_\Omega |X|^p \, dP \right)^{1/p} < \infty \quad \text{and} \quad \mathbb{E}[X] = 0.
	\end{align*}
\end{definition}

\begin{proposition}[Direct Sum Decomposition of $L^p$]
	The space $L^p(\Omega, \mathcal{F}, P)$ admits the direct sum decomposition
	\begin{align*}
		L^p(\Omega, \mathcal{F}, P) = L^p_0(\Omega, \mathcal{F}, P) \oplus \mathbb{R}.
	\end{align*}
	Every \(X \in L^p\) can be uniquely written as
	\begin{align*}
		X = (X - \mathbb{E}[X]) + \mathbb{E}[X],
	\end{align*}
	where \(X - \mathbb{E}[X] \in L^p_0\) and \(\mathbb{E}[X]\) is a constant.
\end{proposition}

\begin{remark}
	When $p = 2$, the space $L^2$ is a Hilbert space with inner product $\langle X, Y \rangle = \mathbb{E}[XY]$, and $L^2_0$ is indeed the orthogonal complement of \(\mathbb{R}\) since \(\langle X, \mathbf{1} \rangle = \mathbb{E}[X] = 0\) for all \(X \in L^2_0\). For \(p \neq 2\), we cannot speak of orthogonal complements as \(L^p\) lacks a canonical inner product structure.
\end{remark}

\begin{lemma}[$L^p_0$ is Closed]
	The subspace $L^p_0(\Omega, \mathcal{F}, P)$ is closed in the $L^p$ norm topology.
\end{lemma}

\begin{proof}
	Let $\{X_n\}$ be a sequence in $L^p_0$ such that $X_n \to X$ in $L^p$ norm. We need to show $\mathbb{E}[X] = 0$. By Hölder's inequality with $q$ such that $\frac{1}{p} + \frac{1}{q} = 1$:
	\begin{align*}
		|\mathbb{E}[X] - \mathbb{E}[X_n]| = |\mathbb{E}[X - X_n]| \leq \|X - X_n\|_p \|\mathbf{1}\|_q = \|X - X_n\|_p \cdot P(\Omega)^{1/q}.
	\end{align*}
	Since $P(\Omega) = 1$ and $\mathbb{E}[X_n] = 0$ for all $n$, taking $n \to \infty$ gives $\mathbb{E}[X] = 0$.
\end{proof}

\begin{remark}[Projection Operator Perspective]
	The decomposition can also be understood by noticing that the expectation operator \(\mathbb{E}: L^p \to \mathbb{R}\) is a bounded linear projection operator with \(\ker(\mathbb{E}) = L^p_0\) and \(\text{range}(\mathbb{E}) = \mathbb{R}\). The centering operator \(\Pi_{L^p_0}: L^p \to L^p_0\), given by \(X \mapsto X - \mathbb{E}[X]\), is the complementary projection onto \(L^p_0\).
\end{remark}

\begin{remark}[Topological Direct Sum]
	Since \(L^p\) is a Banach space and \(L^p_0\) is a closed subspace, the decomposition \(L^p = L^p_0 \oplus \mathbb{R}\) is a \emph{topological direct sum}. This means:
	\begin{enumerate}[label=(\roman*)]
		\item The sum is closed: \(L^p_0 + \mathbb{R} = L^p\) is closed.
		\item The intersection is trivial: \(L^p_0 \cap \mathbb{R} = \{0\}\).
		\item The projection operators are bounded (continuous).
	\end{enumerate}
	Moreover, the projection norms satisfy $\|E\|_{\text{op}} = 1$ and $\|I - E\|_{\text{op}} = 1$ where $I$ is the identity operator.
\end{remark}

\begin{remark}[Annihilator and Dual Perspective]
	For $1 < p < \infty$, the dual space $(L^p)^* \cong L^q$ where $\frac{1}{p} + \frac{1}{q} = 1$. Each $Y \in L^q$ defines a continuous linear functional $\phi_Y: L^p \to \mathbb{R}$ by $\phi_Y(X) = \mathbb{E}[XY]$. The annihilator of $L^p_0$ in $L^q$ is
	\begin{align*}
		(L^p_0)^\perp = \{Y \in L^q : \mathbb{E}[XY] = 0 \text{ for all } X \in L^p_0\}.
	\end{align*}
	Since any $X \in L^p_0$ has $\mathbb{E}[X] = 0$, we can show that $(L^p_0)^\perp = \mathbb{R} \cap L^q$, which is exactly the space of constant functions in $L^q$. This duality relationship provides another characterization of the decomposition.
\end{remark}

\begin{definition}[Centered \(p\)-th Moment]
	The centered \(p\)-th moment of a random variable \(X\) is defined as the \(p\)-th moment of the centered random variable \(X^\circ = \Pi_{L^p_0}(X) = X - \mathbb{E}[X]\),
	\begin{align*}
		\left \| X^\circ \right \|_p =
		\left \| \Pi_{L^p_0}(X) \right\|_p = \left \| X - \mathbb{E}[X] \right\|_p .
	\end{align*}
\end{definition}

\begin{definition}[Moment Generating Function]
	The moment generating function of a random variable \(X\) is defined as
	\begin{align*}
		M_X(t) = \mathbb{E}[e^{tX}].
	\end{align*}
\end{definition}

\begin{todo}
	\itshape Understand why we care about the moment generating function.
\end{todo}

We now introduce some standard notation for common moments.

\begin{definition}[Standard Moments and Properties]
	For a random variable \(X\), we define:
	\begin{enumerate}[label=(\roman*)]
		\item \textbf{Variance:}
		      \begin{align*}
			      \operatorname{Var}(X) := \mathbb{E}\left[ \Pi_{L^2_0}(X)^2 \right] = \mathbb{E}[(X - \mathbb{E}[X])^2] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2.
		      \end{align*}

		\item \textbf{Standard Deviation:}
		      \begin{align*}
			      \sigma(X) := \sqrt{\operatorname{Var}(X)} = \sqrt{\mathbb{E}\left[ \Pi_{L^2_0}(X)^2 \right]}.
		      \end{align*}

		\item \textbf{Skewness:}
		      \begin{align*}
			      \operatorname{Skew}(X) := \frac{\mathbb{E}\left[ \Pi_{L^3_0}(X)^3 \right]}{\operatorname{Var}(X)^{3/2}}.
		      \end{align*}

		\item \textbf{Kurtosis:}
		      \begin{align*}
			      \operatorname{Kurt}(X) := \frac{\mathbb{E}\left[ \Pi_{L^4_0}(X)^4 \right]}{\operatorname{Var}(X)^2}.
		      \end{align*}

		\item \textbf{Excess Kurtosis:}
		      \begin{align*}
			      \operatorname{ExKurt}(X) := \operatorname{Kurt}(X) - 3.
		      \end{align*}

		\item A random variable \(X\) is said to be \textbf{leptokurtic} if \(\operatorname{ExKurt}(X) > 0\).

		\item A random variable \(X\) is said to be \textbf{platykurtic} if \(\operatorname{ExKurt}(X) < 0\).
	\end{enumerate}
\end{definition}

\subsection{\(\sigma\)-Algebras and Conditional Probability}

\begin{definition}[\(\sigma\)-Algebra]
	A \(\sigma\)-algebra \(\mathcal{F}\) on a set \(\Omega\) is a collection of subsets of \(\Omega\) that satisfies:
	\begin{enumerate}[label=(\roman*)]
		\item \(\Omega \in \mathcal{F}\).
		\item If \(A \in \mathcal{F}\), then \(\Omega \setminus A \in \mathcal{F}\).
		\item If \(A_1, A_2, \ldots \in \mathcal{F}\), then \(\bigcup_{i=1}^\infty A_i \in \mathcal{F}\).
	\end{enumerate}
\end{definition}

\begin{definition}[\(\sigma\)-Algebra Generated by a Set of Events]
	Let \(\mathcal{G}\) be a collection of subsets of \(\Omega\).
	The \(\sigma\)-algebra generated by \(\mathcal{G}\), denoted \(\sigma(\mathcal{G})\), is the smallest \(\sigma\)-algebra on \(\Omega\) that contains \(\mathcal{G}\).
	Equivalently, it is the intersection of all \(\sigma\)-algebras on \(\Omega\) that contain \(\mathcal{G}\),
	\begin{align*}
		\sigma(\mathcal{G}) = \bigcap_{\substack{\mathcal{F} \supseteq \mathcal{G} \\\mathcal{F}\text{ a }\sigma\text{-algebra} \text{ on }\Omega}} \mathcal{F}.
	\end{align*}
\end{definition}

\begin{definition}[\(\sigma\)-Algebra Generated by a Random Variable]
	Given a random variable $X$, the \(\sigma\)-algebra generated by $X$, denoted \(\sigma(X)\), is the \(\sigma\)-algebra generated by the collection of sets \( \{X^{-1}(B) : B \in \mathcal{B}(\mathbb{R})\} \), where \(\mathcal{B}(\mathbb{R})\) is the Borel \(\sigma\)-algebra on \(\mathbb{R}\).
\end{definition}

% Todo:
sigma algebras represent information or knowledge of information.
- Relate to clayton2023bernoulli
- Give an explicit example of a filtration representing historical price information for a time series.

\begin{definition}
	Let $\mathcal{G} \subseteq \mathcal{F}$ be a sub-$\sigma$-algebra. The \emph{conditional expectation} of $X$ given $\mathcal{G}$, denoted $\mathbb{E}[X|\mathcal{G}]$, is the unique (up to almost sure equality) $\mathcal{G}$-measurable function satisfying
	\[
		\int_G \mathbb{E}[X|\mathcal{G}] \, dP = \int_G X \, dP, \quad \forall G \in \mathcal{G}.
	\]
\end{definition}

\begin{remark}
	In $L^2(\Omega, \mathcal{F}, P)$, $\mathbb{E}[X|\mathcal{G}]$ coincides with the orthogonal projection of $X$ onto the closed subspace of $\mathcal{G}$-measurable square-integrable random variables.
\end{remark}

\section{\texorpdfstring{$L^p$}{Lp} Spaces and Equivalence Classes}

\begin{definition}
	For $1 \leq p < \infty$, the space $L^p(\Omega, \mathcal{F}, P)$ consists of equivalence classes of measurable functions $X$ with finite $p$-th moment:
	\[
		\|X\|_p := \left( \int_\Omega |X|^p \, dP \right)^{1/p} < \infty.
	\]
\end{definition}

\begin{definition}
	Two random variables $X, Y$ are said to be \emph{equivalent} if $P(X \neq Y)=0$. Each element of $L^p$ is an equivalence class $[X]$ of almost surely equal functions.
\end{definition}

\begin{lemma}[Existence of Borel-Measurable Representatives]
	Let $X \in L^p(\Omega, \mathcal{F}, P)$. There exists a Borel-measurable representative $Y \in [X]$ that coincides with $X$ almost surely. However, no canonical representative exists in general.
\end{lemma}

\begin{note}
	In applications, representatives with additional regularity, such as continuity or cadlag paths, are often chosen to facilitate analysis and computations.
\end{note}

\begin{proposition}[Radon-Nikodym Derivative as Probability Density]
	Let $X$ be a random variable and let $P_X = X_*P$ denote its distribution. If $P_X \ll \lambda$ (the Lebesgue measure), then there exists a density function $f_X \in L^1(\mathbb{R})$ such that
	\[
		P_X(B) = \int_B f_X(x) \, dx, \quad \forall B \in \mathcal{B}(\mathbb{R}).
	\]
	This $f_X = \frac{dP_X}{d\lambda}$ is the \emph{probability density function} (pdf) of $X$.
\end{proposition}

% \section{Inner Product Structure and Covariance}
\subsection{Return Space and Covariance}

The most important random variables of study are the returns of financial assets, portfolios, and trading strategies.

\begin{definition}
	The space $L^2(\Omega, \mathcal{F}, P)$ is a Hilbert space with inner product
	\[
		\langle X, Y \rangle := \int_\Omega X Y \, dP.
	\]
\end{definition}

\begin{definition}
	For centered random variables (i.e., $\mathbb{E}[X]=0$), the \emph{covariance} is
	\[
		\operatorname{Cov}(X, Y) = \langle X, Y \rangle.
	\]
\end{definition}

\section{Moments, Generating Functions, and \(L^2_0\)}

\begin{definition}
	The \emph{$n$-th moment} of a random variable $X$ is $\mathbb{E}[X^n]$, provided the expectation exists.
\end{definition}

\begin{theorem}[Hölder's Inequality]
	Let $1 \leq p, q \leq \infty$ with $\frac{1}{p} + \frac{1}{q} = 1$. Then
	\[
		|\mathbb{E}[XY]| \leq \|X\|_p \|Y\|_q.
	\]
\end{theorem}

\begin{definition}
	The \emph{moment generating function} (MGF) of $X$, if it exists in a neighborhood of zero, is
	\[
		M_X(t) := \mathbb{E}[e^{tX}].
	\]
\end{definition}

\section{Modes of Convergence}

\begin{definition}
	Let $\{X_n\}$ be a sequence of random variables. We distinguish several modes of convergence:
	\begin{enumerate}[label=(\roman*)]
		\item \emph{Almost sure convergence}: $X_n(\omega) \to X(\omega)$ for $P$-almost all $\omega$.
		\item \emph{Convergence in probability}: $\forall \varepsilon>0$, $P(|X_n - X| > \varepsilon) \to 0$.
		\item \emph{$L^p$ convergence}: $\|X_n - X\|_p \to 0$.
		\item \emph{Convergence in distribution}: $P_{X_n} \to P_X$ weakly.
	\end{enumerate}
\end{definition}

\begin{theorem}[Prokhorov's Theorem]
	A family of probability measures on a Polish space is tight if and only if it is relatively compact in the topology of weak convergence.
\end{theorem}

\begin{theorem}[Skorokhod's Representation Theorem]
	Let $P_{X_n} \to P_X$ weakly. Then there exist random variables $\{\tilde{X}_n\}$ and $\tilde{X}$ on a common probability space such that $\tilde{X}_n \xrightarrow{a.s.} \tilde{X}$ and $\tilde{X}_n$ has the same distribution as $X_n$ for each $n$.
\end{theorem}

\section{Functional Analytic Perspectives}

\begin{theorem}[Riesz Representation Theorem]
	Let $1 < p < \infty$. Every continuous linear functional on $L^p(\Omega, \mathcal{F}, P)$ is of the form
	\[
		\varphi(X) = \int_\Omega X Y \, dP
	\]
	for some $Y \in L^q(\Omega, \mathcal{F}, P)$ with $\frac{1}{p} + \frac{1}{q}=1$.
\end{theorem}

\begin{remark}
	This perspective frames the expectation as a continuous linear functional and highlights the duality between $L^p$ and $L^q$ spaces.
\end{remark}


% -----------------------------------------------------------
% REFERENCES
\begin{thebibliography}{9}

	\bibitem{ref1}
	Author Name, \emph{Title of Paper}, Journal Name, vol. XX, pp. 1--10, 20XX.

\end{thebibliography}

\end{document}
